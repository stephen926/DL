{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87681d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "ctrl_folder = r'D:\\DL\\data_train'\n",
    "spr_folder = r'D:\\DL\\data_spread'\n",
    "\n",
    "suffix = 'grb2'\n",
    "files = os.listdir(ctrl_folder)\n",
    "files_000 = [f for f in files if f.endswith('_f000.grb2')]\n",
    "files_006 = [f for f in files if f.endswith('_f006.grb2')]\n",
    "file_ctrl_00 = [os.path.join(ctrl_folder, name) for name in files_000]\n",
    "file_ctrl_06 = [os.path.join(ctrl_folder, name) for name in files_006]\n",
    "# print(file_ctrl_00)\n",
    "\n",
    "files = os.listdir(spr_folder)\n",
    "file_spr = [f for f in files if f.endswith(suffix)]\n",
    "file_spr = [os.path.join(spr_folder, name) for name in file_spr]\n",
    "# print(file_spr)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b6aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from grbdata import grbdata\n",
    "from tsf2sfm import spharm_transform, spectral_to_grid\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, ctrl_filepaths_00, ctrl_filepaths_06, spread_filepaths, layer, X_mean=None, X_std=None, Y_mean=None, Y_std=None):\n",
    "        assert len(ctrl_filepaths_06) == len(spread_filepaths), \"文件数量不一致\"\n",
    "        self.ctrl_filepaths_00 = ctrl_filepaths_00\n",
    "        self.ctrl_filepaths_06 = ctrl_filepaths_06\n",
    "        self.spread_filepaths = spread_filepaths\n",
    "        self.layer = layer\n",
    "        self.X_mean = X_mean\n",
    "        self.X_std = X_std\n",
    "        self.Y_mean = Y_mean\n",
    "        self.Y_std = Y_std\n",
    "\n",
    "        # 控制变量顺序（不包含lon和lat，因为spharm_transform会删除它们）\n",
    "        self.ctrl_vars = ['gh','t','r','u','v',\n",
    "                          'gh_diff','t_diff','r_diff','u_diff','v_diff',\n",
    "                          'gh_grad','t_grad','r_grad','u_grad','v_grad',\n",
    "                          'div_ctrl','vor_ctrl']\n",
    "        self.spread_vars = ['gh', 't', 'r', 'u', 'v']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ctrl_filepaths_00)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ctrl_path_00 = self.ctrl_filepaths_00[idx]\n",
    "        ctrl_path_06 = self.ctrl_filepaths_06[idx]\n",
    "        spread_path = self.spread_filepaths[idx]\n",
    "\n",
    "        ctrl_data, spread_data = grbdata(ctrl_path_00, ctrl_path_06, spread_path, self.layer, zoomin=1, ds=1)\n",
    "        # 对数据进行球谐变换\n",
    "        ctrl_data, spread_data = spharm_transform(ctrl_data), spharm_transform(spread_data)\n",
    "\n",
    "        # 安全地构建数组，只使用存在的变量\n",
    "        try:\n",
    "            X = np.stack([ctrl_data[k] for k in self.ctrl_vars], axis=0)\n",
    "            Y = np.stack([spread_data[k] for k in self.spread_vars], axis=0)\n",
    "        except KeyError as e:\n",
    "            print(f\"缺少变量 {e}\")\n",
    "            print(f\"ctrl_data可用变量: {list(ctrl_data.keys())}\")\n",
    "            print(f\"spread_data可用变量: {list(spread_data.keys())}\")\n",
    "            raise\n",
    "        \n",
    "        # print(X.shape, Y.shape)\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e71dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: X.shape = torch.Size([17, 16471]), Y.shape = torch.Size([5, 16471])\n"
     ]
    }
   ],
   "source": [
    "dataset = WeatherDataset(file_ctrl_00, file_ctrl_06, file_spr, layer=500)\n",
    "# 获取第一个 batch\n",
    "data_iter = iter(dataset)\n",
    "X_batch, Y_batch = next(data_iter)\n",
    "print(f\"Batch: X.shape = {X_batch.shape}, Y.shape = {Y_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e303b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SphericalHarmonics1DCNN(nn.Module):\n",
    "    def __init__(self, input_vars=17, output_vars=5, sph_coeffs=16471):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1D卷积处理球谐系数维度\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_vars, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, output_vars, kernel_size=5, padding=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 17, 16471)\n",
    "        x = self.conv_layers(x)\n",
    "        # output shape: (batch_size, 5, 16471)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7615e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, device='cuda'):\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # 记录训练过程\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, (X, Y) in enumerate(train_loader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪（可选，防止梯度爆炸）\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # 打印进度\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, Y)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.6f}')\n",
    "        print(f'  Val Loss: {avg_val_loss:.6f}')\n",
    "        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.8f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def split_dataset_sklearn(dataset, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    使用sklearn划分数据集\n",
    "    \"\"\"\n",
    "    # 获取数据集索引\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    # 划分索引\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # 创建子数据集\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# 使用示例\n",
    "train_dataset, val_dataset = split_dataset_sklearn(dataset, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048aed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total parameters: 89,221\n",
      "Trainable parameters: 89,221\n",
      "Epoch 1/100, Batch 0, Loss: 0.116874\n",
      "Epoch 1/100, Batch 10, Loss: 0.001658\n",
      "Epoch 1/100, Batch 20, Loss: 0.000559\n",
      "Epoch 1/100:\n",
      "  Train Loss: 0.008749\n",
      "  Val Loss: 0.000306\n",
      "  Learning Rate: 0.00099975\n",
      "--------------------------------------------------\n",
      "Epoch 2/100, Batch 0, Loss: 0.000291\n",
      "Epoch 2/100, Batch 10, Loss: 0.000233\n",
      "Epoch 2/100, Batch 20, Loss: 0.000220\n",
      "Epoch 2/100:\n",
      "  Train Loss: 0.000238\n",
      "  Val Loss: 0.000214\n",
      "  Learning Rate: 0.00099901\n",
      "--------------------------------------------------\n",
      "Epoch 3/100, Batch 0, Loss: 0.000202\n",
      "Epoch 3/100, Batch 10, Loss: 0.000201\n",
      "Epoch 3/100, Batch 20, Loss: 0.000198\n",
      "Epoch 3/100:\n",
      "  Train Loss: 0.000206\n",
      "  Val Loss: 0.000199\n",
      "  Learning Rate: 0.00099778\n",
      "--------------------------------------------------\n",
      "Epoch 4/100, Batch 0, Loss: 0.000186\n",
      "Epoch 4/100, Batch 10, Loss: 0.000181\n",
      "Epoch 4/100, Batch 20, Loss: 0.000175\n",
      "Epoch 4/100:\n",
      "  Train Loss: 0.000188\n",
      "  Val Loss: 0.000177\n",
      "  Learning Rate: 0.00099606\n",
      "--------------------------------------------------\n",
      "Epoch 5/100, Batch 0, Loss: 0.000163\n",
      "Epoch 5/100, Batch 10, Loss: 0.000155\n",
      "Epoch 5/100, Batch 20, Loss: 0.000150\n",
      "Epoch 5/100:\n",
      "  Train Loss: 0.000163\n",
      "  Val Loss: 0.000152\n",
      "  Learning Rate: 0.00099384\n",
      "--------------------------------------------------\n",
      "Epoch 6/100, Batch 0, Loss: 0.000136\n",
      "Epoch 6/100, Batch 10, Loss: 0.000128\n",
      "Epoch 6/100, Batch 20, Loss: 0.000133\n",
      "Epoch 6/100:\n",
      "  Train Loss: 0.000141\n",
      "  Val Loss: 0.000133\n",
      "  Learning Rate: 0.00099114\n",
      "--------------------------------------------------\n",
      "Epoch 7/100, Batch 0, Loss: 0.000118\n",
      "Epoch 7/100, Batch 10, Loss: 0.000116\n",
      "Epoch 7/100, Batch 20, Loss: 0.000121\n",
      "Epoch 7/100:\n",
      "  Train Loss: 0.000129\n",
      "  Val Loss: 0.000126\n",
      "  Learning Rate: 0.00098796\n",
      "--------------------------------------------------\n",
      "Epoch 8/100, Batch 0, Loss: 0.000114\n",
      "Epoch 8/100, Batch 10, Loss: 0.000102\n",
      "Epoch 8/100, Batch 20, Loss: 0.000113\n",
      "Epoch 8/100:\n",
      "  Train Loss: 0.000119\n",
      "  Val Loss: 0.000118\n",
      "  Learning Rate: 0.00098429\n",
      "--------------------------------------------------\n",
      "Epoch 9/100, Batch 0, Loss: 0.000106\n",
      "Epoch 9/100, Batch 10, Loss: 0.000099\n",
      "Epoch 9/100, Batch 20, Loss: 0.000108\n",
      "Epoch 9/100:\n",
      "  Train Loss: 0.000114\n",
      "  Val Loss: 0.000114\n",
      "  Learning Rate: 0.00098015\n",
      "--------------------------------------------------\n",
      "Epoch 10/100, Batch 0, Loss: 0.000102\n",
      "Epoch 10/100, Batch 10, Loss: 0.000094\n",
      "Epoch 10/100, Batch 20, Loss: 0.000105\n",
      "Epoch 10/100:\n",
      "  Train Loss: 0.000110\n",
      "  Val Loss: 0.000110\n",
      "  Learning Rate: 0.00097553\n",
      "--------------------------------------------------\n",
      "Epoch 11/100, Batch 0, Loss: 0.000099\n",
      "Epoch 11/100, Batch 10, Loss: 0.000092\n",
      "Epoch 11/100, Batch 20, Loss: 0.000104\n",
      "Epoch 11/100:\n",
      "  Train Loss: 0.000108\n",
      "  Val Loss: 0.000108\n",
      "  Learning Rate: 0.00097044\n",
      "--------------------------------------------------\n",
      "Epoch 12/100, Batch 0, Loss: 0.000097\n",
      "Epoch 12/100, Batch 10, Loss: 0.000091\n",
      "Epoch 12/100, Batch 20, Loss: 0.000103\n",
      "Epoch 12/100:\n",
      "  Train Loss: 0.000111\n",
      "  Val Loss: 0.000111\n",
      "  Learning Rate: 0.00096489\n",
      "--------------------------------------------------\n",
      "Epoch 13/100, Batch 0, Loss: 0.000098\n",
      "Epoch 13/100, Batch 10, Loss: 0.000088\n",
      "Epoch 13/100, Batch 20, Loss: 0.000100\n",
      "Epoch 13/100:\n",
      "  Train Loss: 0.000107\n",
      "  Val Loss: 0.000105\n",
      "  Learning Rate: 0.00095888\n",
      "--------------------------------------------------\n",
      "Epoch 14/100, Batch 0, Loss: 0.000093\n",
      "Epoch 14/100, Batch 10, Loss: 0.000087\n",
      "Epoch 14/100, Batch 20, Loss: 0.000094\n",
      "Epoch 14/100:\n",
      "  Train Loss: 0.000102\n",
      "  Val Loss: 0.000102\n",
      "  Learning Rate: 0.00095241\n",
      "--------------------------------------------------\n",
      "Epoch 15/100, Batch 0, Loss: 0.000090\n",
      "Epoch 15/100, Batch 10, Loss: 0.000086\n",
      "Epoch 15/100, Batch 20, Loss: 0.000096\n",
      "Epoch 15/100:\n",
      "  Train Loss: 0.000101\n",
      "  Val Loss: 0.000101\n",
      "  Learning Rate: 0.00094550\n",
      "--------------------------------------------------\n",
      "Epoch 16/100, Batch 0, Loss: 0.000089\n",
      "Epoch 16/100, Batch 10, Loss: 0.000087\n",
      "Epoch 16/100, Batch 20, Loss: 0.000093\n",
      "Epoch 16/100:\n",
      "  Train Loss: 0.000098\n",
      "  Val Loss: 0.000099\n",
      "  Learning Rate: 0.00093815\n",
      "--------------------------------------------------\n",
      "Epoch 17/100, Batch 0, Loss: 0.000088\n",
      "Epoch 17/100, Batch 10, Loss: 0.000084\n",
      "Epoch 17/100, Batch 20, Loss: 0.000092\n",
      "Epoch 17/100:\n",
      "  Train Loss: 0.000097\n",
      "  Val Loss: 0.000099\n",
      "  Learning Rate: 0.00093037\n",
      "--------------------------------------------------\n",
      "Epoch 18/100, Batch 0, Loss: 0.000087\n",
      "Epoch 18/100, Batch 10, Loss: 0.000083\n",
      "Epoch 18/100, Batch 20, Loss: 0.000092\n",
      "Epoch 18/100:\n",
      "  Train Loss: 0.000098\n",
      "  Val Loss: 0.000100\n",
      "  Learning Rate: 0.00092216\n",
      "--------------------------------------------------\n",
      "Epoch 19/100, Batch 0, Loss: 0.000088\n",
      "Epoch 19/100, Batch 10, Loss: 0.000082\n",
      "Epoch 19/100, Batch 20, Loss: 0.000090\n",
      "Epoch 19/100:\n",
      "  Train Loss: 0.000095\n",
      "  Val Loss: 0.000097\n",
      "  Learning Rate: 0.00091354\n",
      "--------------------------------------------------\n",
      "Epoch 20/100, Batch 0, Loss: 0.000085\n",
      "Epoch 20/100, Batch 10, Loss: 0.000081\n",
      "Epoch 20/100, Batch 20, Loss: 0.000089\n",
      "Epoch 20/100:\n",
      "  Train Loss: 0.000094\n",
      "  Val Loss: 0.000096\n",
      "  Learning Rate: 0.00090451\n",
      "--------------------------------------------------\n",
      "Epoch 21/100, Batch 0, Loss: 0.000084\n",
      "Epoch 21/100, Batch 10, Loss: 0.000080\n",
      "Epoch 21/100, Batch 20, Loss: 0.000089\n",
      "Epoch 21/100:\n",
      "  Train Loss: 0.000093\n",
      "  Val Loss: 0.000096\n",
      "  Learning Rate: 0.00089508\n",
      "--------------------------------------------------\n",
      "Epoch 22/100, Batch 0, Loss: 0.000083\n",
      "Epoch 22/100, Batch 10, Loss: 0.000080\n",
      "Epoch 22/100, Batch 20, Loss: 0.000089\n",
      "Epoch 22/100:\n",
      "  Train Loss: 0.000092\n",
      "  Val Loss: 0.000096\n",
      "  Learning Rate: 0.00088526\n",
      "--------------------------------------------------\n",
      "Epoch 23/100, Batch 0, Loss: 0.000082\n",
      "Epoch 23/100, Batch 10, Loss: 0.000078\n",
      "Epoch 23/100, Batch 20, Loss: 0.000089\n",
      "Epoch 23/100:\n",
      "  Train Loss: 0.000091\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00087506\n",
      "--------------------------------------------------\n",
      "Epoch 24/100, Batch 0, Loss: 0.000082\n",
      "Epoch 24/100, Batch 10, Loss: 0.000079\n",
      "Epoch 24/100, Batch 20, Loss: 0.000088\n",
      "Epoch 24/100:\n",
      "  Train Loss: 0.000090\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00086448\n",
      "--------------------------------------------------\n",
      "Epoch 25/100, Batch 0, Loss: 0.000082\n",
      "Epoch 25/100, Batch 10, Loss: 0.000078\n",
      "Epoch 25/100, Batch 20, Loss: 0.000087\n",
      "Epoch 25/100:\n",
      "  Train Loss: 0.000090\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00085355\n",
      "--------------------------------------------------\n",
      "Epoch 26/100, Batch 0, Loss: 0.000081\n",
      "Epoch 26/100, Batch 10, Loss: 0.000077\n",
      "Epoch 26/100, Batch 20, Loss: 0.000086\n",
      "Epoch 26/100:\n",
      "  Train Loss: 0.000089\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00084227\n",
      "--------------------------------------------------\n",
      "Epoch 27/100, Batch 0, Loss: 0.000080\n",
      "Epoch 27/100, Batch 10, Loss: 0.000076\n",
      "Epoch 27/100, Batch 20, Loss: 0.000087\n",
      "Epoch 27/100:\n",
      "  Train Loss: 0.000088\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00083066\n",
      "--------------------------------------------------\n",
      "Epoch 28/100, Batch 0, Loss: 0.000080\n",
      "Epoch 28/100, Batch 10, Loss: 0.000076\n",
      "Epoch 28/100, Batch 20, Loss: 0.000086\n",
      "Epoch 28/100:\n",
      "  Train Loss: 0.000088\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00081871\n",
      "--------------------------------------------------\n",
      "Epoch 29/100, Batch 0, Loss: 0.000080\n",
      "Epoch 29/100, Batch 10, Loss: 0.000075\n",
      "Epoch 29/100, Batch 20, Loss: 0.000086\n",
      "Epoch 29/100:\n",
      "  Train Loss: 0.000087\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00080645\n",
      "--------------------------------------------------\n",
      "Epoch 30/100, Batch 0, Loss: 0.000079\n",
      "Epoch 30/100, Batch 10, Loss: 0.000075\n",
      "Epoch 30/100, Batch 20, Loss: 0.000085\n",
      "Epoch 30/100:\n",
      "  Train Loss: 0.000087\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00079389\n",
      "--------------------------------------------------\n",
      "Epoch 31/100, Batch 0, Loss: 0.000079\n",
      "Epoch 31/100, Batch 10, Loss: 0.000075\n",
      "Epoch 31/100, Batch 20, Loss: 0.000086\n",
      "Epoch 31/100:\n",
      "  Train Loss: 0.000086\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00078104\n",
      "--------------------------------------------------\n",
      "Epoch 32/100, Batch 0, Loss: 0.000079\n",
      "Epoch 32/100, Batch 10, Loss: 0.000074\n",
      "Epoch 32/100, Batch 20, Loss: 0.000084\n",
      "Epoch 32/100:\n",
      "  Train Loss: 0.000086\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00076791\n",
      "--------------------------------------------------\n",
      "Epoch 33/100, Batch 0, Loss: 0.000079\n",
      "Epoch 33/100, Batch 10, Loss: 0.000074\n",
      "Epoch 33/100, Batch 20, Loss: 0.000084\n",
      "Epoch 33/100:\n",
      "  Train Loss: 0.000086\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00075452\n",
      "--------------------------------------------------\n",
      "Epoch 34/100, Batch 0, Loss: 0.000079\n",
      "Epoch 34/100, Batch 10, Loss: 0.000074\n",
      "Epoch 34/100, Batch 20, Loss: 0.000083\n",
      "Epoch 34/100:\n",
      "  Train Loss: 0.000085\n",
      "  Val Loss: 0.000096\n",
      "  Learning Rate: 0.00074088\n",
      "--------------------------------------------------\n",
      "Epoch 35/100, Batch 0, Loss: 0.000080\n",
      "Epoch 35/100, Batch 10, Loss: 0.000074\n",
      "Epoch 35/100, Batch 20, Loss: 0.000082\n",
      "Epoch 35/100:\n",
      "  Train Loss: 0.000084\n",
      "  Val Loss: 0.000096\n",
      "  Learning Rate: 0.00072700\n",
      "--------------------------------------------------\n",
      "Epoch 36/100, Batch 0, Loss: 0.000080\n",
      "Epoch 36/100, Batch 10, Loss: 0.000074\n",
      "Epoch 36/100, Batch 20, Loss: 0.000082\n",
      "Epoch 36/100:\n",
      "  Train Loss: 0.000084\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00071289\n",
      "--------------------------------------------------\n",
      "Epoch 37/100, Batch 0, Loss: 0.000079\n",
      "Epoch 37/100, Batch 10, Loss: 0.000074\n",
      "Epoch 37/100, Batch 20, Loss: 0.000081\n",
      "Epoch 37/100:\n",
      "  Train Loss: 0.000083\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00069857\n",
      "--------------------------------------------------\n",
      "Epoch 38/100, Batch 0, Loss: 0.000079\n",
      "Epoch 38/100, Batch 10, Loss: 0.000074\n",
      "Epoch 38/100, Batch 20, Loss: 0.000080\n",
      "Epoch 38/100:\n",
      "  Train Loss: 0.000082\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00068406\n",
      "--------------------------------------------------\n",
      "Epoch 39/100, Batch 0, Loss: 0.000078\n",
      "Epoch 39/100, Batch 10, Loss: 0.000074\n",
      "Epoch 39/100, Batch 20, Loss: 0.000080\n",
      "Epoch 39/100:\n",
      "  Train Loss: 0.000082\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00066937\n",
      "--------------------------------------------------\n",
      "Epoch 40/100, Batch 0, Loss: 0.000078\n",
      "Epoch 40/100, Batch 10, Loss: 0.000074\n",
      "Epoch 40/100, Batch 20, Loss: 0.000080\n",
      "Epoch 40/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000093\n",
      "  Learning Rate: 0.00065451\n",
      "--------------------------------------------------\n",
      "Epoch 41/100, Batch 0, Loss: 0.000077\n",
      "Epoch 41/100, Batch 10, Loss: 0.000073\n",
      "Epoch 41/100, Batch 20, Loss: 0.000079\n",
      "Epoch 41/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000093\n",
      "  Learning Rate: 0.00063950\n",
      "--------------------------------------------------\n",
      "Epoch 42/100, Batch 0, Loss: 0.000077\n",
      "Epoch 42/100, Batch 10, Loss: 0.000073\n",
      "Epoch 42/100, Batch 20, Loss: 0.000081\n",
      "Epoch 42/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00062434\n",
      "--------------------------------------------------\n",
      "Epoch 43/100, Batch 0, Loss: 0.000079\n",
      "Epoch 43/100, Batch 10, Loss: 0.000073\n",
      "Epoch 43/100, Batch 20, Loss: 0.000078\n",
      "Epoch 43/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00060907\n",
      "--------------------------------------------------\n",
      "Epoch 44/100, Batch 0, Loss: 0.000078\n",
      "Epoch 44/100, Batch 10, Loss: 0.000074\n",
      "Epoch 44/100, Batch 20, Loss: 0.000078\n",
      "Epoch 44/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000093\n",
      "  Learning Rate: 0.00059369\n",
      "--------------------------------------------------\n",
      "Epoch 45/100, Batch 0, Loss: 0.000077\n",
      "Epoch 45/100, Batch 10, Loss: 0.000074\n",
      "Epoch 45/100, Batch 20, Loss: 0.000079\n",
      "Epoch 45/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00057822\n",
      "--------------------------------------------------\n",
      "Epoch 46/100, Batch 0, Loss: 0.000077\n",
      "Epoch 46/100, Batch 10, Loss: 0.000074\n",
      "Epoch 46/100, Batch 20, Loss: 0.000079\n",
      "Epoch 46/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00056267\n",
      "--------------------------------------------------\n",
      "Epoch 47/100, Batch 0, Loss: 0.000077\n",
      "Epoch 47/100, Batch 10, Loss: 0.000074\n",
      "Epoch 47/100, Batch 20, Loss: 0.000079\n",
      "Epoch 47/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00054705\n",
      "--------------------------------------------------\n",
      "Epoch 48/100, Batch 0, Loss: 0.000077\n",
      "Epoch 48/100, Batch 10, Loss: 0.000074\n",
      "Epoch 48/100, Batch 20, Loss: 0.000080\n",
      "Epoch 48/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000093\n",
      "  Learning Rate: 0.00053140\n",
      "--------------------------------------------------\n",
      "Epoch 49/100, Batch 0, Loss: 0.000076\n",
      "Epoch 49/100, Batch 10, Loss: 0.000073\n",
      "Epoch 49/100, Batch 20, Loss: 0.000079\n",
      "Epoch 49/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000092\n",
      "  Learning Rate: 0.00051571\n",
      "--------------------------------------------------\n",
      "Epoch 50/100, Batch 0, Loss: 0.000074\n",
      "Epoch 50/100, Batch 10, Loss: 0.000072\n",
      "Epoch 50/100, Batch 20, Loss: 0.000079\n",
      "Epoch 50/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000092\n",
      "  Learning Rate: 0.00050000\n",
      "--------------------------------------------------\n",
      "Epoch 51/100, Batch 0, Loss: 0.000074\n",
      "Epoch 51/100, Batch 10, Loss: 0.000071\n",
      "Epoch 51/100, Batch 20, Loss: 0.000079\n",
      "Epoch 51/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000090\n",
      "  Learning Rate: 0.00048429\n",
      "--------------------------------------------------\n",
      "Epoch 52/100, Batch 0, Loss: 0.000072\n",
      "Epoch 52/100, Batch 10, Loss: 0.000070\n",
      "Epoch 52/100, Batch 20, Loss: 0.000078\n",
      "Epoch 52/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00046860\n",
      "--------------------------------------------------\n",
      "Epoch 53/100, Batch 0, Loss: 0.000072\n",
      "Epoch 53/100, Batch 10, Loss: 0.000070\n",
      "Epoch 53/100, Batch 20, Loss: 0.000077\n",
      "Epoch 53/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000088\n",
      "  Learning Rate: 0.00045295\n",
      "--------------------------------------------------\n",
      "Epoch 54/100, Batch 0, Loss: 0.000072\n",
      "Epoch 54/100, Batch 10, Loss: 0.000071\n",
      "Epoch 54/100, Batch 20, Loss: 0.000077\n",
      "Epoch 54/100:\n",
      "  Train Loss: 0.000081\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00043733\n",
      "--------------------------------------------------\n",
      "Epoch 55/100, Batch 0, Loss: 0.000073\n",
      "Epoch 55/100, Batch 10, Loss: 0.000070\n",
      "Epoch 55/100, Batch 20, Loss: 0.000077\n",
      "Epoch 55/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000091\n",
      "  Learning Rate: 0.00042178\n",
      "--------------------------------------------------\n",
      "Epoch 56/100, Batch 0, Loss: 0.000075\n",
      "Epoch 56/100, Batch 10, Loss: 0.000070\n",
      "Epoch 56/100, Batch 20, Loss: 0.000077\n",
      "Epoch 56/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000094\n",
      "  Learning Rate: 0.00040631\n",
      "--------------------------------------------------\n",
      "Epoch 57/100, Batch 0, Loss: 0.000078\n",
      "Epoch 57/100, Batch 10, Loss: 0.000071\n",
      "Epoch 57/100, Batch 20, Loss: 0.000075\n",
      "Epoch 57/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000095\n",
      "  Learning Rate: 0.00039093\n",
      "--------------------------------------------------\n",
      "Epoch 58/100, Batch 0, Loss: 0.000079\n",
      "Epoch 58/100, Batch 10, Loss: 0.000073\n",
      "Epoch 58/100, Batch 20, Loss: 0.000073\n",
      "Epoch 58/100:\n",
      "  Train Loss: 0.000080\n",
      "  Val Loss: 0.000092\n",
      "  Learning Rate: 0.00037566\n",
      "--------------------------------------------------\n",
      "Epoch 59/100, Batch 0, Loss: 0.000076\n",
      "Epoch 59/100, Batch 10, Loss: 0.000071\n",
      "Epoch 59/100, Batch 20, Loss: 0.000073\n",
      "Epoch 59/100:\n",
      "  Train Loss: 0.000079\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00036050\n",
      "--------------------------------------------------\n",
      "Epoch 60/100, Batch 0, Loss: 0.000074\n",
      "Epoch 60/100, Batch 10, Loss: 0.000069\n",
      "Epoch 60/100, Batch 20, Loss: 0.000073\n",
      "Epoch 60/100:\n",
      "  Train Loss: 0.000078\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00034549\n",
      "--------------------------------------------------\n",
      "Epoch 61/100, Batch 0, Loss: 0.000073\n",
      "Epoch 61/100, Batch 10, Loss: 0.000069\n",
      "Epoch 61/100, Batch 20, Loss: 0.000073\n",
      "Epoch 61/100:\n",
      "  Train Loss: 0.000078\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00033063\n",
      "--------------------------------------------------\n",
      "Epoch 62/100, Batch 0, Loss: 0.000073\n",
      "Epoch 62/100, Batch 10, Loss: 0.000068\n",
      "Epoch 62/100, Batch 20, Loss: 0.000073\n",
      "Epoch 62/100:\n",
      "  Train Loss: 0.000090\n",
      "  Val Loss: 0.000112\n",
      "  Learning Rate: 0.00031594\n",
      "--------------------------------------------------\n",
      "Epoch 63/100, Batch 0, Loss: 0.000100\n",
      "Epoch 63/100, Batch 10, Loss: 0.000085\n",
      "Epoch 63/100, Batch 20, Loss: 0.000075\n",
      "Epoch 63/100:\n",
      "  Train Loss: 0.000088\n",
      "  Val Loss: 0.000091\n",
      "  Learning Rate: 0.00030143\n",
      "--------------------------------------------------\n",
      "Epoch 64/100, Batch 0, Loss: 0.000076\n",
      "Epoch 64/100, Batch 10, Loss: 0.000071\n",
      "Epoch 64/100, Batch 20, Loss: 0.000074\n",
      "Epoch 64/100:\n",
      "  Train Loss: 0.000079\n",
      "  Val Loss: 0.000090\n",
      "  Learning Rate: 0.00028711\n",
      "--------------------------------------------------\n",
      "Epoch 65/100, Batch 0, Loss: 0.000075\n",
      "Epoch 65/100, Batch 10, Loss: 0.000069\n",
      "Epoch 65/100, Batch 20, Loss: 0.000073\n",
      "Epoch 65/100:\n",
      "  Train Loss: 0.000078\n",
      "  Val Loss: 0.000090\n",
      "  Learning Rate: 0.00027300\n",
      "--------------------------------------------------\n",
      "Epoch 66/100, Batch 0, Loss: 0.000075\n",
      "Epoch 66/100, Batch 10, Loss: 0.000069\n",
      "Epoch 66/100, Batch 20, Loss: 0.000074\n",
      "Epoch 66/100:\n",
      "  Train Loss: 0.000077\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00025912\n",
      "--------------------------------------------------\n",
      "Epoch 67/100, Batch 0, Loss: 0.000074\n",
      "Epoch 67/100, Batch 10, Loss: 0.000069\n",
      "Epoch 67/100, Batch 20, Loss: 0.000074\n",
      "Epoch 67/100:\n",
      "  Train Loss: 0.000077\n",
      "  Val Loss: 0.000089\n",
      "  Learning Rate: 0.00024548\n",
      "--------------------------------------------------\n",
      "Epoch 68/100, Batch 0, Loss: 0.000074\n",
      "Epoch 68/100, Batch 10, Loss: 0.000069\n",
      "Epoch 68/100, Batch 20, Loss: 0.000074\n",
      "Epoch 68/100:\n",
      "  Train Loss: 0.000077\n",
      "  Val Loss: 0.000088\n",
      "  Learning Rate: 0.00023209\n",
      "--------------------------------------------------\n",
      "Epoch 69/100, Batch 0, Loss: 0.000073\n",
      "Epoch 69/100, Batch 10, Loss: 0.000070\n",
      "Epoch 69/100, Batch 20, Loss: 0.000073\n",
      "Epoch 69/100:\n",
      "  Train Loss: 0.000077\n",
      "  Val Loss: 0.000088\n",
      "  Learning Rate: 0.00021896\n",
      "--------------------------------------------------\n",
      "Epoch 70/100, Batch 0, Loss: 0.000073\n",
      "Epoch 70/100, Batch 10, Loss: 0.000068\n",
      "Epoch 70/100, Batch 20, Loss: 0.000074\n",
      "Epoch 70/100:\n",
      "  Train Loss: 0.000077\n",
      "  Val Loss: 0.000087\n",
      "  Learning Rate: 0.00020611\n",
      "--------------------------------------------------\n",
      "Epoch 71/100, Batch 0, Loss: 0.000073\n",
      "Epoch 71/100, Batch 10, Loss: 0.000067\n",
      "Epoch 71/100, Batch 20, Loss: 0.000074\n",
      "Epoch 71/100:\n",
      "  Train Loss: 0.000076\n",
      "  Val Loss: 0.000087\n",
      "  Learning Rate: 0.00019355\n",
      "--------------------------------------------------\n",
      "Epoch 72/100, Batch 0, Loss: 0.000072\n",
      "Epoch 72/100, Batch 10, Loss: 0.000067\n",
      "Epoch 72/100, Batch 20, Loss: 0.000074\n",
      "Epoch 72/100:\n",
      "  Train Loss: 0.000076\n",
      "  Val Loss: 0.000086\n",
      "  Learning Rate: 0.00018129\n",
      "--------------------------------------------------\n",
      "Epoch 73/100, Batch 0, Loss: 0.000072\n",
      "Epoch 73/100, Batch 10, Loss: 0.000067\n",
      "Epoch 73/100, Batch 20, Loss: 0.000074\n",
      "Epoch 73/100:\n",
      "  Train Loss: 0.000076\n",
      "  Val Loss: 0.000086\n",
      "  Learning Rate: 0.00016934\n",
      "--------------------------------------------------\n",
      "Epoch 74/100, Batch 0, Loss: 0.000072\n",
      "Epoch 74/100, Batch 10, Loss: 0.000066\n",
      "Epoch 74/100, Batch 20, Loss: 0.000074\n",
      "Epoch 74/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000086\n",
      "  Learning Rate: 0.00015773\n",
      "--------------------------------------------------\n",
      "Epoch 75/100, Batch 0, Loss: 0.000071\n",
      "Epoch 75/100, Batch 10, Loss: 0.000065\n",
      "Epoch 75/100, Batch 20, Loss: 0.000073\n",
      "Epoch 75/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000086\n",
      "  Learning Rate: 0.00014645\n",
      "--------------------------------------------------\n",
      "Epoch 76/100, Batch 0, Loss: 0.000071\n",
      "Epoch 76/100, Batch 10, Loss: 0.000065\n",
      "Epoch 76/100, Batch 20, Loss: 0.000073\n",
      "Epoch 76/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000086\n",
      "  Learning Rate: 0.00013552\n",
      "--------------------------------------------------\n",
      "Epoch 77/100, Batch 0, Loss: 0.000071\n",
      "Epoch 77/100, Batch 10, Loss: 0.000065\n",
      "Epoch 77/100, Batch 20, Loss: 0.000074\n",
      "Epoch 77/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000085\n",
      "  Learning Rate: 0.00012494\n",
      "--------------------------------------------------\n",
      "Epoch 78/100, Batch 0, Loss: 0.000070\n",
      "Epoch 78/100, Batch 10, Loss: 0.000065\n",
      "Epoch 78/100, Batch 20, Loss: 0.000075\n",
      "Epoch 78/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000085\n",
      "  Learning Rate: 0.00011474\n",
      "--------------------------------------------------\n",
      "Epoch 79/100, Batch 0, Loss: 0.000069\n",
      "Epoch 79/100, Batch 10, Loss: 0.000065\n",
      "Epoch 79/100, Batch 20, Loss: 0.000076\n",
      "Epoch 79/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000085\n",
      "  Learning Rate: 0.00010492\n",
      "--------------------------------------------------\n",
      "Epoch 80/100, Batch 0, Loss: 0.000069\n",
      "Epoch 80/100, Batch 10, Loss: 0.000065\n",
      "Epoch 80/100, Batch 20, Loss: 0.000077\n",
      "Epoch 80/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000085\n",
      "  Learning Rate: 0.00009549\n",
      "--------------------------------------------------\n",
      "Epoch 81/100, Batch 0, Loss: 0.000069\n",
      "Epoch 81/100, Batch 10, Loss: 0.000064\n",
      "Epoch 81/100, Batch 20, Loss: 0.000076\n",
      "Epoch 81/100:\n",
      "  Train Loss: 0.000075\n",
      "  Val Loss: 0.000085\n",
      "  Learning Rate: 0.00008646\n",
      "--------------------------------------------------\n",
      "Epoch 82/100, Batch 0, Loss: 0.000069\n",
      "Epoch 82/100, Batch 10, Loss: 0.000065\n",
      "Epoch 82/100, Batch 20, Loss: 0.000074\n",
      "Epoch 82/100:\n",
      "  Train Loss: 0.000074\n",
      "  Val Loss: 0.000084\n",
      "  Learning Rate: 0.00007784\n",
      "--------------------------------------------------\n",
      "Epoch 83/100, Batch 0, Loss: 0.000069\n",
      "Epoch 83/100, Batch 10, Loss: 0.000065\n",
      "Epoch 83/100, Batch 20, Loss: 0.000073\n",
      "Epoch 83/100:\n",
      "  Train Loss: 0.000074\n",
      "  Val Loss: 0.000084\n",
      "  Learning Rate: 0.00006963\n",
      "--------------------------------------------------\n",
      "Epoch 84/100, Batch 0, Loss: 0.000069\n",
      "Epoch 84/100, Batch 10, Loss: 0.000064\n",
      "Epoch 84/100, Batch 20, Loss: 0.000073\n",
      "Epoch 84/100:\n",
      "  Train Loss: 0.000074\n",
      "  Val Loss: 0.000084\n",
      "  Learning Rate: 0.00006185\n",
      "--------------------------------------------------\n",
      "Epoch 85/100, Batch 0, Loss: 0.000069\n",
      "Epoch 85/100, Batch 10, Loss: 0.000064\n",
      "Epoch 85/100, Batch 20, Loss: 0.000072\n",
      "Epoch 85/100:\n",
      "  Train Loss: 0.000074\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00005450\n",
      "--------------------------------------------------\n",
      "Epoch 86/100, Batch 0, Loss: 0.000069\n",
      "Epoch 86/100, Batch 10, Loss: 0.000064\n",
      "Epoch 86/100, Batch 20, Loss: 0.000072\n",
      "Epoch 86/100:\n",
      "  Train Loss: 0.000073\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00004759\n",
      "--------------------------------------------------\n",
      "Epoch 87/100, Batch 0, Loss: 0.000069\n",
      "Epoch 87/100, Batch 10, Loss: 0.000063\n",
      "Epoch 87/100, Batch 20, Loss: 0.000072\n",
      "Epoch 87/100:\n",
      "  Train Loss: 0.000073\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00004112\n",
      "--------------------------------------------------\n",
      "Epoch 88/100, Batch 0, Loss: 0.000068\n",
      "Epoch 88/100, Batch 10, Loss: 0.000063\n",
      "Epoch 88/100, Batch 20, Loss: 0.000072\n",
      "Epoch 88/100:\n",
      "  Train Loss: 0.000073\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00003511\n",
      "--------------------------------------------------\n",
      "Epoch 89/100, Batch 0, Loss: 0.000068\n",
      "Epoch 89/100, Batch 10, Loss: 0.000063\n",
      "Epoch 89/100, Batch 20, Loss: 0.000072\n",
      "Epoch 89/100:\n",
      "  Train Loss: 0.000073\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00002956\n",
      "--------------------------------------------------\n",
      "Epoch 90/100, Batch 0, Loss: 0.000068\n",
      "Epoch 90/100, Batch 10, Loss: 0.000062\n",
      "Epoch 90/100, Batch 20, Loss: 0.000072\n",
      "Epoch 90/100:\n",
      "  Train Loss: 0.000072\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00002447\n",
      "--------------------------------------------------\n",
      "Epoch 91/100, Batch 0, Loss: 0.000068\n",
      "Epoch 91/100, Batch 10, Loss: 0.000062\n",
      "Epoch 91/100, Batch 20, Loss: 0.000071\n",
      "Epoch 91/100:\n",
      "  Train Loss: 0.000072\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00001985\n",
      "--------------------------------------------------\n",
      "Epoch 92/100, Batch 0, Loss: 0.000068\n",
      "Epoch 92/100, Batch 10, Loss: 0.000063\n",
      "Epoch 92/100, Batch 20, Loss: 0.000071\n",
      "Epoch 92/100:\n",
      "  Train Loss: 0.000072\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00001571\n",
      "--------------------------------------------------\n",
      "Epoch 93/100, Batch 0, Loss: 0.000068\n",
      "Epoch 93/100, Batch 10, Loss: 0.000063\n",
      "Epoch 93/100, Batch 20, Loss: 0.000071\n",
      "Epoch 93/100:\n",
      "  Train Loss: 0.000072\n",
      "  Val Loss: 0.000083\n",
      "  Learning Rate: 0.00001204\n",
      "--------------------------------------------------\n",
      "Epoch 94/100, Batch 0, Loss: 0.000068\n",
      "Epoch 94/100, Batch 10, Loss: 0.000063\n",
      "Epoch 94/100, Batch 20, Loss: 0.000071\n",
      "Epoch 94/100:\n",
      "  Train Loss: 0.000072\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000886\n",
      "--------------------------------------------------\n",
      "Epoch 95/100, Batch 0, Loss: 0.000068\n",
      "Epoch 95/100, Batch 10, Loss: 0.000063\n",
      "Epoch 95/100, Batch 20, Loss: 0.000071\n",
      "Epoch 95/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000616\n",
      "--------------------------------------------------\n",
      "Epoch 96/100, Batch 0, Loss: 0.000067\n",
      "Epoch 96/100, Batch 10, Loss: 0.000063\n",
      "Epoch 96/100, Batch 20, Loss: 0.000071\n",
      "Epoch 96/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000394\n",
      "--------------------------------------------------\n",
      "Epoch 97/100, Batch 0, Loss: 0.000067\n",
      "Epoch 97/100, Batch 10, Loss: 0.000063\n",
      "Epoch 97/100, Batch 20, Loss: 0.000071\n",
      "Epoch 97/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000222\n",
      "--------------------------------------------------\n",
      "Epoch 98/100, Batch 0, Loss: 0.000067\n",
      "Epoch 98/100, Batch 10, Loss: 0.000064\n",
      "Epoch 98/100, Batch 20, Loss: 0.000070\n",
      "Epoch 98/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000099\n",
      "--------------------------------------------------\n",
      "Epoch 99/100, Batch 0, Loss: 0.000067\n",
      "Epoch 99/100, Batch 10, Loss: 0.000064\n",
      "Epoch 99/100, Batch 20, Loss: 0.000070\n",
      "Epoch 99/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000025\n",
      "--------------------------------------------------\n",
      "Epoch 100/100, Batch 0, Loss: 0.000067\n",
      "Epoch 100/100, Batch 10, Loss: 0.000064\n",
      "Epoch 100/100, Batch 20, Loss: 0.000070\n",
      "Epoch 100/100:\n",
      "  Train Loss: 0.000071\n",
      "  Val Loss: 0.000082\n",
      "  Learning Rate: 0.00000000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # 创建数据加载器（假设你已经有了dataset）\n",
    "    # train_dataset = train_dataset # 你的数据集\n",
    "    # val_dataset = val_dataset\n",
    "    \n",
    "    # train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = SphericalHarmonics1DCNN(input_vars=17, output_vars=5, sph_coeffs=16471)\n",
    "    \n",
    "    # 检查模型参数数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Total parameters: {total_params:,}')\n",
    "    print(f'Trainable parameters: {trainable_params:,}')\n",
    "    \n",
    "    # 开始训练\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_dataset, val_dataset, \n",
    "        num_epochs=100, device=device\n",
    "    )\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1c206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 17, 181, 360]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m spr_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgefs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mspr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgespr.t00z.pgrb2a.0p50.f006.grb2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# 调用预测\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m \u001b[43mpredict_new_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl_test_00\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl_test_06\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspr_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_diff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 87\u001b[0m, in \u001b[0;36mpredict_new_file\u001b[1;34m(ctrl_path_00, ctrl_path_06, spr_path, layer, model, plot_diff)\u001b[0m\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 87\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, varname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(spread_vars):\n\u001b[0;32m     90\u001b[0m     resplot(Y[i], varname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue Spread\u001b[39m\u001b[38;5;124m\"\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m\"\u001b[39m, extend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mSphericalHarmonics1DCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# x shape: (batch_size, 17, 16471)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# output shape: (batch_size, 5, 16471)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 17, 181, 360]"
     ]
    }
   ],
   "source": [
    "# 可视化函数\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import spharm\n",
    "from matplotlib import pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from UNet import UNet\n",
    "import torch\n",
    "\n",
    "now_filename = \"best_model.pth\"\n",
    "# 模型结构与加载\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SphericalHarmonics1DCNN().to(device)\n",
    "# print(device)\n",
    "checkpoint = torch.load(now_filename, map_location=device)\n",
    "print(now_filename)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def resplot(var, variablename, titlename, cmap, extend, save=False, savepath='D://DL//train//', n_keep=None):\n",
    "    \"\"\"\n",
    "    绘图函数，支持球谐滤波。\n",
    "    - var: 输入变量，shape = (361, 720)，纬向从北到南，经向从0到360\n",
    "    - n_keep: 球谐保留的最大 mode 数；若为 None 则不滤波\n",
    "    \"\"\"\n",
    "    \n",
    "    # 开始绘图\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    proj = ccrs.PlateCarree(central_longitude=180)\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.8, 0.6], projection=proj)\n",
    "    ax.set_global()\n",
    "    ax.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.set_xticks(np.arange(-180, 181, 30), crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(np.arange(-90, 91, 30), crs=ccrs.PlateCarree())\n",
    "    ax.xaxis.set_major_formatter(cticker.LongitudeFormatter())\n",
    "    ax.yaxis.set_major_formatter(cticker.LatitudeFormatter())\n",
    "\n",
    "    lon, lat = np.linspace(0, 360, var.shape[1]), np.linspace(90, -90, var.shape[0])\n",
    "    x, y = np.meshgrid(lon - 180, lat)\n",
    "\n",
    "    # 设置等值线级别\n",
    "    if variablename == 'gh':\n",
    "        levels = np.arange(0, 31, 2)\n",
    "    elif variablename == 't':\n",
    "        levels = np.arange(0, 3.5, 0.5)\n",
    "    elif variablename == 'r':\n",
    "        levels = np.arange(0, 40, 5)\n",
    "    elif variablename in ['u', 'v']:\n",
    "        levels = np.arange(0, 11, 2)\n",
    "    else:\n",
    "        levels = None\n",
    "\n",
    "    if titlename == 'Difference':\n",
    "        levels = None\n",
    "\n",
    "    c = ax.contourf(x, y, var, levels=levels, cmap=cmap, extend=extend, transform=ccrs.PlateCarree())\n",
    "    plt.colorbar(c, ax=ax, orientation=\"vertical\", shrink=0.75)\n",
    "    plt.title(f\"{variablename} {titlename}\")\n",
    "\n",
    "    if save:\n",
    "        savefile = f\"{savepath}{variablename}_{titlename.replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
    "        plt.savefig(savefile, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "from grbdata import grbdata\n",
    "# 预测函数（使用 grbdata 函数）\n",
    "def predict_new_file(ctrl_path_00, ctrl_path_06, spr_path, layer, model, plot_diff=True):\n",
    "    data_ctrl, data_spread = grbdata(ctrl_path_00, ctrl_path_06, spr_path, layer)\n",
    "\n",
    "    ctrl_vars = ['gh','t','r','u','v','gh_diff','t_diff','r_diff','u_diff','v_diff','gh_grad','t_grad','r_grad','u_grad','v_grad','div_ctrl','vor_ctrl']\n",
    "    spread_vars = ['gh', 't', 'r', 'u', 'v']\n",
    "\n",
    "    X = np.stack([data_ctrl[k] for k in ctrl_vars], axis=0)\n",
    "    Y = np.stack([data_spread[k] for k in spread_vars], axis=0)\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred = model(X_tensor).squeeze(0).cpu().numpy()\n",
    "\n",
    "    for i, varname in enumerate(spread_vars):\n",
    "        resplot(Y[i], varname, \"True Spread\", cmap=\"Blues\", extend=\"max\")\n",
    "        resplot(Y_pred[i], varname, \"Predicted Spread\", cmap=\"Blues\", extend=\"max\")\n",
    "        n_keep = int(1e4)\n",
    "        resplot(Y[i], varname, \"True Spread\", cmap=\"Blues\", extend=\"max\",n_keep = n_keep)\n",
    "        # resplot(Y_pred[i], varname, \"Predicted Spread\", cmap=\"Blues\", extend=\"max\",n_keep = n_keep)\n",
    "        if plot_diff:\n",
    "            resplot((Y[i] - Y_pred[i])/Y[i], varname, \"Difference\", cmap=\"Reds\", extend=\"both\")\n",
    "            resplot((Y[i] - Y_pred[i])/Y[i], varname, \"Difference\", cmap=\"Reds\", extend=\"both\",n_keep = n_keep)\n",
    "            continue\n",
    "\n",
    "test_date = 20250609\n",
    "\n",
    "ctrl_test_00 = f'D:\\DL\\data_train\\{test_date}_gec00_f000.grb2'\n",
    "ctrl_test_06 = f'D:\\DL\\data_train\\{test_date}_gec00_f006.grb2'\n",
    "spr_test = f\"D:\\DL\\data_spread\\{test_date}_gespr_f006.grb2\"\n",
    "\n",
    "ctrl_test_00 = f'D:\\DL\\gefs\\ctrl\\gec00.t00z.pgrb2a.0p50.f000.grb2'\n",
    "ctrl_test_06 = f'D:\\DL\\gefs\\ctrl\\gec00.t00z.pgrb2a.0p50.f006.grb2'\n",
    "spr_test = f'D:\\DL\\gefs\\spr\\gespr.t00z.pgrb2a.0p50.f006.grb2'\n",
    "\n",
    "# 调用预测\n",
    "predict_new_file(ctrl_test_00, ctrl_test_06, spr_test, 500, model, plot_diff = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
